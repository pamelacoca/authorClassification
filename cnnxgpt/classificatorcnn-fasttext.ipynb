{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import nltk.tokenize as nt\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "from torch.utils.data import (TensorDataset, DataLoader, RandomSampler,\n",
    "                              SequentialSampler)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import device\n",
    "import torch.optim as optim\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import datasetgenerator as dsg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == \"mps\":\n",
    "    dtype_value_torch = dtype=torch.float32\n",
    "    dtype_value_np = dtype=np.float32\n",
    "else:\n",
    "    dtype_value_torch = dtype=torch.float64\n",
    "    dtype_value_np = dtype=np.float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTHOR1 = \"coutinho-dataset\"\n",
    "AUTHOR2 = \"denser-dataset\"\n",
    "PATH_TO_RAW_DATA = \"data/raw/\"\n",
    "PATH_TO_PARSED_DATA = \"data/parsed/\"\n",
    "\n",
    "ds_gen = dsg.ds_gen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_data = ds_gen.get_data_length(dsg.ds_gen.DENSER, go_up_on_path=1)\n",
    "print(len_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictAuthors = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_number(dict, word):\n",
    "    if(word in dict.keys()):\n",
    "        return dict.get(word)\n",
    "    else:\n",
    "        next_value = len(dict.keys())\n",
    "        dict.update({word: next_value})\n",
    "        return next_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictAuthors.update({\"padding\": 0})\n",
    "map_to_number(dictAuthors, \"unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_words_in_paragraph(paragraph, dict):\n",
    "    tensor = []\n",
    "    parsed_paragraph = nt.word_tokenize(paragraph)\n",
    "    for word in parsed_paragraph:\n",
    "        token = map_to_number(dict, word)\n",
    "        tensor.append(token)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paragraph_size_list(paragraph_set):\n",
    "    paragraph_sizes = []\n",
    "    for paragraph in paragraph_set:\n",
    "        paragraph_size = len(paragraph)\n",
    "        paragraph_sizes.append(paragraph_size)\n",
    "    return paragraph_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_data = ds_gen.get_data_length(dsg.ds_gen.DENSER, go_up_on_path=1)\n",
    "print(len_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset_1, empty_dataset_1 = ds_gen.get_dataset_from_author(dsg.ds_gen.COUTINHO, 1, len_data, go_up_on_path=1)\n",
    "full_dataset_2, empty_dataset_2 = ds_gen.get_dataset_from_author(dsg.ds_gen.DENSER, 1, len_data, go_up_on_path=1)\n",
    "type(full_dataset_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_json(filepath, filename):\n",
    "    with open(os.path.join(filepath, filename), \"r\", encoding=\"utf8\") as f:\n",
    "        loaded_data = json.load(f)\n",
    "        return loaded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_train_coutinho = get_data_from_json(\"../data/gpt_train_parsed/\",\"gpt_coutinho.json\")\n",
    "gpt_train_denser = get_data_from_json(\"../data/gpt_train_parsed/\",\"gpt_denser.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_test_coutinho = get_data_from_json(\"../data/gpt_test_parsed/\",\"gpt_coutinho.json\")\n",
    "gpt_test_denser = get_data_from_json(\"../data/gpt_test_parsed/\",\"gpt_denser.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_keys_from_dataset(dataset, keys):\n",
    "    for key in keys:\n",
    "        if(dataset.get(key) is not None):\n",
    "            dataset.pop(key)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_dataset_1 = remove_keys_from_dataset(full_dataset_1, gpt_train_coutinho.keys())\n",
    "reduced_dataset_1 = remove_keys_from_dataset(reduced_dataset_1, gpt_test_coutinho.keys())\n",
    "reduced_dataset_2 = remove_keys_from_dataset(full_dataset_2, gpt_train_denser.keys())\n",
    "reduced_dataset_2 = remove_keys_from_dataset(reduced_dataset_2, gpt_test_denser.keys())\n",
    "len(reduced_dataset_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion_training = 0.8\n",
    "len_available_data = len_data-80\n",
    "training_part = int(proportion_training * len_available_data)\n",
    "test_part = len_available_data - training_part\n",
    "print(training_part)\n",
    "print(test_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_dataset_randomly(dataset, quantity, avoid_indices=[]):\n",
    "\n",
    "    shuffled_keys = list(dataset.keys())\n",
    "    for key in dataset.keys():\n",
    "        if(key in avoid_indices):\n",
    "            shuffled_keys.remove(key)\n",
    "    random.shuffle(shuffled_keys)\n",
    "\n",
    "    if(len(shuffled_keys) >= quantity):\n",
    "        selected_data = {}\n",
    "        for index in range(quantity):\n",
    "            key = shuffled_keys[index]\n",
    "            selected_data.update({key: dataset[key]})\n",
    "        return selected_data\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_test_dataset_1 = segment_dataset_randomly(reduced_dataset_1, test_part)\n",
    "selected_training_dataset_1 = segment_dataset_randomly(reduced_dataset_1, training_part-len(gpt_train_coutinho), list(selected_test_dataset_1.keys()))\n",
    "selected_test_dataset_2 = segment_dataset_randomly(reduced_dataset_2, test_part)\n",
    "selected_training_dataset_2 = segment_dataset_randomly(reduced_dataset_2, training_part-len(gpt_train_denser), list(selected_test_dataset_2.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_training_dataset_1 = list(selected_training_dataset_1.values())\n",
    "complete_training_dataset_1.extend(list(gpt_train_coutinho.values()))\n",
    "complete_training_dataset_2 = list(selected_training_dataset_2.values())\n",
    "complete_training_dataset_2.extend(list(gpt_train_denser.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dt_1_sizes = get_paragraph_size_list(complete_training_dataset_1)\n",
    "train_dt_2_sizes = get_paragraph_size_list(complete_training_dataset_2)\n",
    "test_dt_1_sizes = get_paragraph_size_list(list(selected_test_dataset_1.values()))\n",
    "test_dt_2_sizes = get_paragraph_size_list(list(selected_test_dataset_2.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_raw_both = copy.deepcopy(complete_training_dataset_1)\n",
    "train_dataset_raw_both.extend(complete_training_dataset_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_raw_both = copy.deepcopy(list(selected_test_dataset_1.values()))\n",
    "test_dataset_raw_both.extend(list(selected_test_dataset_2.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_labels = [0]*len(complete_training_dataset_1)\n",
    "train_labels_2 = [1]*len(complete_training_dataset_2)\n",
    "\n",
    "train_labels.extend(train_labels_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = [0]*len(selected_test_dataset_1)\n",
    "test_labels_2 = [1]*len(selected_test_dataset_2)\n",
    "\n",
    "test_labels.extend(test_labels_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = copy.deepcopy(train_dt_1_sizes)\n",
    "all_data.extend(train_dt_2_sizes)\n",
    "all_data.extend(test_dt_1_sizes)\n",
    "all_data.extend(test_dt_2_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(all_data)\n",
    "percetile = np.percentile(all_data, 85)\n",
    "print(percetile)\n",
    "plt.axhline(y=percetile, color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dt_2_sizes = get_paragraph_size_list(complete_training_dataset_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(data, rule):\n",
    "    while (len(data) < rule):\n",
    "        data.append(0)\n",
    "\n",
    "    if(len(data) > rule):\n",
    "        data = data[0:rule]\n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_paragraph_set(paragraph_set):\n",
    "    tokenized_paragraph_set= []\n",
    "    for paragraph in paragraph_set:\n",
    "        lowered_paragraph = paragraph.lower()\n",
    "        tokenized_paragraph = tokenize_words_in_paragraph(lowered_paragraph, dictAuthors)\n",
    "        tokenized_paragraph = normalize_data(tokenized_paragraph, 520)\n",
    "        tokenized_paragraph_set.append(tokenized_paragraph)\n",
    "    return tokenized_paragraph_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_set1 = tokenize_paragraph_set(complete_training_dataset_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_set2 = tokenize_paragraph_set(complete_training_dataset_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_both_tokenized = copy.deepcopy(tokenized_set1)\n",
    "train_dataset_both_tokenized.extend(tokenized_set2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test1 = tokenize_paragraph_set(list(selected_test_dataset_1.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test2 = tokenize_paragraph_set(list(selected_test_dataset_2.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_both_tokenized = copy.deepcopy(tokenized_test1)\n",
    "test_dataset_both_tokenized.extend(tokenized_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('authorsDict.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(dictAuthors, f, ensure_ascii=False, indent=4)\n",
    "f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_vectors(word2idx, fname):\n",
    "    \n",
    "    print(\"Loading pretrained vectors...\")\n",
    "    fin = open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "\n",
    "    print(n)\n",
    "    print(d)\n",
    "\n",
    "    embeddings = np.random.uniform(-0.25, 0.25, (len(word2idx), d))\n",
    "    embeddings[word2idx['padding']] = np.zeros((d,))\n",
    "\n",
    "    count = 0\n",
    "    for line in fin:\n",
    "        #print(line)\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        word = tokens[0]\n",
    "        if word in word2idx:\n",
    "            count += 1\n",
    "            embeddings[word2idx[word]] = np.array(tokens[1:], dtype=dtype_value_np)\n",
    "\n",
    "    print(f\"There are {count} / {len(word2idx)} pretrained vectors found.\")\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = load_pretrained_vectors(dictAuthors, \"../cc.pt.300.vec\")\n",
    "embeddings = torch.tensor(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(train_inputs, test_inputs, train_labels, test_labels,\n",
    "                batch_size=50):\n",
    "   \n",
    "    train_inputs, test_inputs, train_labels, test_labels =\\\n",
    "    tuple(torch.tensor(data) for data in\n",
    "          [train_inputs, test_inputs, train_labels, test_labels])\n",
    "\n",
    "    batch_size = 50\n",
    "\n",
    "    train_data = TensorDataset(train_inputs, train_labels)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    test_data = TensorDataset(test_inputs, test_labels)\n",
    "    test_sampler = SequentialSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataloader, test_dataloader = data_loader(train_dataset_both_tokenized, test_dataset_both_tokenized, train_labels, test_labels, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_sizes = [2, 3, 4]\n",
    "num_filters = [2, 2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_NLP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 pretrained_embedding=None,\n",
    "                 freeze_embedding=False,\n",
    "                 vocab_size=None,\n",
    "                 embed_dim=300,\n",
    "                 filter_sizes=[3, 4, 5],\n",
    "                 num_filters=[100, 100, 100],\n",
    "                 num_classes=2,\n",
    "                 dropout=0.5):\n",
    "        \n",
    "        super(CNN_NLP, self).__init__()\n",
    "        \n",
    "        if pretrained_embedding is not None:\n",
    "            self.vocab_size, self.embed_dim = pretrained_embedding.shape\n",
    "            self.embedding = nn.Embedding.from_pretrained(pretrained_embedding,\n",
    "                                                          freeze=freeze_embedding)\n",
    "        else:\n",
    "            self.embed_dim = embed_dim\n",
    "            self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                          embedding_dim=self.embed_dim,\n",
    "                                          padding_idx=0,\n",
    "                                          max_norm=5.0)\n",
    "       \n",
    "        self.conv1d_list = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=self.embed_dim,\n",
    "                      out_channels=num_filters[i],\n",
    "                      kernel_size=filter_sizes[i])\n",
    "            for i in range(len(filter_sizes))\n",
    "        ])\n",
    "       \n",
    "        self.fc = nn.Linear(np.sum(num_filters), num_classes)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \n",
    "        x_embed = self.embedding(input_ids)\n",
    "\n",
    "        x_reshaped = x_embed.permute(0, 2, 1)\n",
    "\n",
    "        x_conv_list = [F.relu(conv1d(x_reshaped)) for conv1d in self.conv1d_list]\n",
    "\n",
    "        x_pool_list = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2])\n",
    "            for x_conv in x_conv_list]\n",
    "        \n",
    "        x_fc = torch.cat([x_pool.squeeze(dim=2) for x_pool in x_pool_list],\n",
    "                         dim=1)\n",
    "        \n",
    "        logits = self.fc(self.dropout(x_fc))\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize_model(pretrained_embedding=None,\n",
    "                    freeze_embedding=False,\n",
    "                    vocab_size=None,\n",
    "                    embed_dim=300,\n",
    "                    filter_sizes=[3, 4, 5],\n",
    "                    num_filters=[100, 100, 100],\n",
    "                    num_classes=2,\n",
    "                    dropout=0.5,\n",
    "                    learning_rate=0.01):\n",
    "\n",
    "    assert (len(filter_sizes) == len(num_filters)), \"filter_sizes and num_filters need to be of the same length.\"\n",
    "\n",
    "    cnn_model = CNN_NLP(pretrained_embedding=pretrained_embedding,\n",
    "                        freeze_embedding=freeze_embedding,\n",
    "                        vocab_size=vocab_size,\n",
    "                        embed_dim=embed_dim,\n",
    "                        filter_sizes=filter_sizes,\n",
    "                        num_filters=num_filters,\n",
    "                        num_classes=2,\n",
    "                        dropout=0.5)\n",
    "\n",
    "    cnn_model.to(device, dtype=dtype_value_torch)\n",
    "\n",
    "    optimizer = optim.Adadelta(cnn_model.parameters(), lr=learning_rate, rho=0.95)\n",
    "\n",
    "    # optimizer = optim.Adam(cnn_model.parameters(),lr=learning_rate, eps=1e-06)\n",
    "    return cnn_model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed_value=42):\n",
    "\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_dataloader):\n",
    "    model.eval()\n",
    "    evaluation_start_time = time.time()\n",
    "    test_accuracy = []\n",
    "    test_loss = []\n",
    "    for batch in test_dataloader:\n",
    "        b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids)\n",
    "\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        test_loss.append(loss.item())\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        test_accuracy.append(accuracy)\n",
    "        \n",
    "    test_loss = np.mean(test_loss)\n",
    "    test_accuracy = np.mean(test_accuracy)\n",
    "\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_dataloader, test_dataloader=None, epochs=10):\n",
    "\n",
    "    best_accuracy = 0\n",
    "    training_start_time = time.time()\n",
    "    test_acc_hist=[]\n",
    "    test_loss_hist=[]\n",
    "    time_elapsed_hist=[]\n",
    "    best_acc_hist=[]\n",
    "\n",
    "\n",
    "    print(\"Start training...\\n\")\n",
    "    print(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Test Loss':^10} | {'Test Acc':^9} | {'Elapsed':^9}\")\n",
    "    print(\"-\"*60)\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "     \n",
    "        t0_epoch = time.time()\n",
    "        total_loss = 0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "           \n",
    "            b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n",
    "            model.zero_grad()\n",
    "            logits = model(b_input_ids)\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        if test_dataloader is not None:\n",
    "            test_loss, test_accuracy = evaluate(model, test_dataloader)\n",
    "            if test_accuracy > best_accuracy:\n",
    "                best_accuracy = test_accuracy\n",
    "                best_acc_hist.append(best_accuracy)\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            print(f\"{epoch_i + 1:^7} | {avg_train_loss:^12.6f} | {test_loss:^10.6f} | {test_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "            \n",
    "        test_acc_hist.append(test_accuracy)\n",
    "        test_loss_hist.append(test_loss)\n",
    "        time_elapsed_hist.append(time_elapsed)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(optimizer)\n",
    "    print(f\"Training complete! Best accuracy: {best_accuracy:.2f}%.\")\n",
    "    print('Training finished, took {:.2f}s'.format(time.time() - training_start_time))\n",
    "\n",
    "    figure, axis = plt.subplots(2, 2)\n",
    "\n",
    "    axis[0, 0].plot(test_acc_hist) \n",
    "\n",
    "    axis[0, 1].plot(test_loss_hist) \n",
    "\n",
    "    axis[1, 0].plot(time_elapsed_hist) \n",
    "  \n",
    "    axis[1, 1].plot(best_acc_hist) \n",
    "\n",
    "    plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "cnn_non_static, optimizer = initialize_model(pretrained_embedding=embeddings,\n",
    "                                            freeze_embedding=False,\n",
    "                                            learning_rate=0.25,\n",
    "                                            dropout=0.5)\n",
    "train(cnn_non_static, optimizer, train_dataloader, test_dataloader, epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictAuthorsOriginal = copy.deepcopy(dictAuthors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_token = dictAuthors.get(\"leu…\")\n",
    "dict_size = len(dictAuthors)\n",
    "bound_size = dict_size - last_token\n",
    "print(dict_size)\n",
    "print(bound_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_words_in_paragraph_for_predict(paragraph, dict):\n",
    "    tensor = []\n",
    "    dict_keys= list(dict.keys())\n",
    "    parsed_paragraph = nt.word_tokenize(paragraph)\n",
    "    for word in parsed_paragraph:\n",
    "        token = map_to_number(dict, word)\n",
    "        if (word not in dict_keys):\n",
    "            token = map_to_number(dict, \"unknown\")\n",
    "        tensor.append(token)\n",
    "    print(tensor) \n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, model=cnn_non_static, max_len=520):\n",
    "\n",
    "    tokens = tokenize_words_in_paragraph_for_predict(text.lower(), dictAuthorsOriginal)\n",
    "    padded_tokens = tokens + [0] * (max_len - len(tokens))\n",
    "\n",
    "    input_id = torch.tensor(padded_tokens).unsqueeze(dim=0)\n",
    "    model.to(device, dtype=dtype_value_torch)\n",
    "\n",
    "    logits = model.forward(input_id.to(device))\n",
    "\n",
    "    probs = F.softmax(logits, dim=1).squeeze(dim=0)\n",
    "\n",
    "    if probs[1] > 0.5:\n",
    "        print(f\"Esse parágrafo tem {probs[1] * 100:.2f}% de chance de ser Denser.\")\n",
    "        return \"Denser\"\n",
    "    else:\n",
    "        chance = 1 -probs[1]\n",
    "        print(f\"Esse parágrafo tem {chance * 100:.2f}% de chance de ser Coutinho.\")\n",
    "        return \"Coutinho\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "# iterate over test data\n",
    "for inputs, labels in test_dataloader:\n",
    "        output = cnn_non_static(inputs.to(device)) # Feed Network\n",
    "\n",
    "        output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()\n",
    "        y_pred.extend(output) # Save Prediction\n",
    "        \n",
    "        labels = labels.data.cpu().numpy()\n",
    "        y_true.extend(labels) # Save Truth\n",
    "\n",
    "# constant for classes\n",
    "classes = ('Coutinho', 'Denser')\n",
    "\n",
    "# Build confusion matrix\n",
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None], index = [i for i in classes],\n",
    "                     columns = [i for i in classes])\n",
    "plt.figure(figsize = (12,7))\n",
    "sn.heatmap(df_cm, annot=True)\n",
    "plt.savefig('output.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(cnn_non_static.state_dict(), \"cnnns.pth\")\n",
    "print(\"Saved PyTorch Model State to cnnns.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cnn_non_static.to(device, dtype=dtype_value_torch)\n",
    "model.load_state_dict(torch.load(\"cnnns.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"Ou cruzes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/gpt_test_parsed/index_map_coutinho.json\") as f:\n",
    "    index_map_0 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/gpt_test_parsed/index_map_denser.json\") as f:\n",
    "    index_map_1 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_gpt_dataset(sorted_result, index_map, gpt_test_dataset):\n",
    "    for i in index_map.keys():\n",
    "        main_index = index_map[i]\n",
    "        prediction = predict(gpt_test_dataset[main_index])\n",
    "        sorted_result.update({int(i): (prediction, gpt_test_dataset[main_index])})\n",
    "    return sorted_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_result = OrderedDict()\n",
    "\n",
    "sorted_result = predict_gpt_dataset(sorted_result, index_map_0, gpt_test_coutinho)\n",
    "sorted_result = predict_gpt_dataset(sorted_result, index_map_1, gpt_test_denser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_indices = sorted(sorted_result)\n",
    "\n",
    "for index in sorted_indices:\n",
    "    print(index, sorted_result[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in sorted_indices:\n",
    "    print(sorted_result[index][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
